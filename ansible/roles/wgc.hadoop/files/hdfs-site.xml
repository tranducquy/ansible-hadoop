<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!--设置为true可使NameNode尝试恢复以前失败的dfs.namenode.name.dir。启用后，将在检查点期间尝试恢复任何失败的目录。-->
    <property>
        <name>dfs.namenode.name.dir.restore</name>
        <value>true</value>
    </property>
<!--    <property>
                <name>dfs.nameservices</name>
        <value>mycluster01</value>
    </property>-->
    <property>
        <name>dfs.nameservices</name>
        <value>mycluster01,mycluster02</value>
    </property>
    <!--mycluster01-->
    <property>
        <name>dfs.ha.namenodes.mycluster01</name>
        <value>nn1,nn2</value>
    </property>
    <!--mycluster02-->
    <property>
        <name>dfs.ha.namenodes.mycluster02</name>
        <value>nn3,nn4</value>
    </property>

    <!--nn1, nn2-->
    <!--Client RPC,namenode处理所有客户端请求的RPC地址和端口,default port 8020-->
    <property>
        <name>dfs.namenode.rpc-address.mycluster01.nn1</name>
        <value>10-6-103-34-vm-jhdxyjd.mob.local:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster01.nn2</name>
        <value>10-6-103-35-vm-jhdxyjd.mob.local:8020</value>
    </property>
    <!--Service RPC,hdfs服务通信的地址端口,若配置了,BackupNode, Datanodes和其他服务因该连接到此端口(上报数据块和心跳状态)
             若没配置将用dfs.namenode.rpc-address作为默认值-->
    <property>
        <name>dfs.namenode.servicerpc-address.mobPrdCluster01.nn1</name>
        <value>10-6-103-34-vm-jhdxyjd.mob.local:8040</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.mobPrdCluster01.nn2</name>
        <value>10-6-103-35-vm-jhdxyjd.mob.local:8040</value>
    </property>
    <!--namenode webui的地址端口，default port 9870 -->
    <property>
        <name>dfs.namenode.http-address.mycluster01.nn1</name>
        <value>10-6-103-34-vm-jhdxyjd.mob.local:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster01.nn2</name>
        <value>10-6-103-35-vm-jhdxyjd.mob.local:9870</value>
    </property>

    <!--nn3, nn4-->
    <property>
        <name>dfs.namenode.rpc-address.mycluster02.nn3</name>
        <value>10-6-103-36-vm-jhdxyjd.mob.local:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.mycluster02.nn4</name>
        <value>10-6-103-37-vm-jhdxyjd.mob.local:8020</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.mobPrdCluster02.nn3</name>
        <value>10-6-103-36-vm-jhdxyjd.mob.local:8040</value>
    </property>
    <property>
        <name>dfs.namenode.servicerpc-address.mobPrdCluster02.nn4</name>
        <value>10-6-103-37-vm-jhdxyjd.mob.local:8040</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster02.nn3</name>
        <value>10-6-103-36-vm-jhdxyjd.mob.local:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.mycluster02.nn4</name>
        <value>10-6-103-37-vm-jhdxyjd.mob.local:9870</value>
    </property>

    <!--QJM 注意：这个配置在2组nameserice中是不一样的
               mycluster01  /mycluster01
      mycluster02  /mycluster02-->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://10-6-103-34-vm-jhdxyjd.mob.local:8485;10-6-103-35-vm-jhdxyjd.mob.local:8485;10-6-103-36-vm-jhdxyjd.mob.local:8485/mycluster01</value>
    </property>
 <!--   <property>
                <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://10-6-103-34-vm-jhdxyjd.mob.local:8485;10-6-103-35-vm-jhdxyjd.mob.local:8485;10-6-103-36-vm-jhdxyjd.mob.local:8485/mycluster02</value>
    </property>-->

    <!--HDFS客户端用来联系Active NameNode的Java类-->
    <property>
        <name>dfs.client.failover.proxy.provider.mycluster01</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.mycluster02</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <!-- List of fencing methods to use for service fencing,fencing的配置官网放在core-site.xml和hdfs-site.xml,为避免混乱，参考
             cdh的配置都放在hdfs-site.xml
    ps:fencing是需要linux安装fuser：yum install -y psmisc-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <!--the SSH private key files to use with the builtin sshfence fencer.-->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/hadoop/.ssh/id_rsa</value>
    </property>
    <!--SSH connection timeout-->
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>10000</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/apache/data/hdfs/jn</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data/apache/data/hdfs/nn</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/apache/data/hdfs/dn</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <!--block size 128MB-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
    <!--Client RPC(dfs.namenode.rpc-address 8020)的线程数-->
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>200</value>
    </property>
    <!--Server RPC(dfs.namenode.servicerpc-address 8040)的线程数-->
    <property>
        <name>dfs.namenode.service.handler.count</name>
        <value>200</value>
    </property>
    <!--写入jn的超时时间，defaut 20000ms-->
    <property>
        <name>dfs.qjournal.write-txns.timeout.ms</name>
        <value>60000</value>
    </property>
    <!--全量块汇报，默认6小时，更改12小时-->
    <property>
        <name>dfs.blockreport.intervalMsec</name>
        <value>43200000</value>
    </property>
    <!--开启增量块汇报-->
    <property>
        <name>dfs.blockreport.incremental.intervalMsec</name>
        <value>300</value>
    </property>
    <!--nn周期性计算dn副本情况的频率单位秒default 3s，此参数在hadoop3.3.0中未找到，疑似废弃-->
    <property>
        <name>dfs.namenode.replication.interval</name>
        <value>10</value>
    </property>
    <!--每个卷的保留空间单位字节默认0 -->
    <property>
        <name>dfs.datanode.du.reserved</name>
        <value>5242880</value>
    </property>
    <!--Does HDFS allow appends to files? hadoop3.3.0未找到此参数-->
    <property>
        <name>dfs.support.append</name>
        <value>true</value>
    </property>
    <!--Enable WebHDFS (REST API) in Namenodes and Datanodes.hadoop3.3.0未找到此参数-->
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
    <!--namenode HA启用自动故障转移-->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <!--datanode间进行数据传输的最大线程数 default 4096-->
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>8192</value>
    </property>
    <!--datanode RPC处理线程数,default 10-->
    <property>
        <name>dfs.datanode.handler.count</name>
        <value>50</value>
    </property>
    <!--datanode节点最多容忍损坏的磁盘数，默认0只要有一个卷故障dn就停止了，此值应大于等-1，-1表示最多允许一个故障-->
<!--    <property>
                <name>dfs.datanode.failed.volumes.tolerated</name>
        <value>2</value>
    </property>-->
    <!--Timeout in ms for clients socket writes to DataNodes.默认480000ms-->
    <property>
        <name>dfs.datanode.socket.write.timeout</name>
        <value>960000</value>
    </property>
    <!--默认enable-->
    <property>
        <name>dfs.disk.balancer.enabled</name>
        <value>true</value>
    </property>
    <!--Maximum disk bandwidth used by diskbalancer during read from a source disk. The unit is MB/sec.默认10MB/s-->
    <property>
        <name>dfs.disk.balancer.max.disk.throughputInMBperSec</name>
        <value>50</value>
    </property>
    <!--在每次移动块的过程中，移动块的数量与理想平衡状态之间的偏差容忍值（按百分比计）。默认10%-->
    <property>
        <name>dfs.disk.balancer.block.tolerance.percent</name>
        <value>10</value>
    </property>
    <!--enable permission checking in HDFS-->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.image.parallel.load</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hadoop</value>
    </property>
    <!--namenode要求必须将连接的datandoe地址解析为hsotname-->
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.balance.bandwidthPerSec</name>
        <value>100m</value>
    </property>
    <!--平衡复制的最大线程数-->
    <property>
        <name>dfs.datanode.balance.max.concurrent.moves</name>
        <value>100</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>true</value>
    </property>
    <!--一个实现ImpersonationProvider接口的类，授权一个用户是否能模拟一个特定的用户，可以指定一个类来决定模拟行为
             如果不指定，默认DefaultImpersonationProvider-->
<!--    <property>
                <name>hadoop.security.impersonation.provider.class</name>
        <value>org.apache.hadoop.security.authorize.my.MyDefaultImpersonationProvider</value>
    </property>-->
    <!--开启异步审计日志-->
    <property>
        <name>dfs.namenode.audit.log.async</name>
        <value>true</value>
    </property>
    <!--开启避免读取stale datanode(nn在指定的时间间隔内没收到该dn发送的心跳信息
             https://community.cloudera.com/t5/Support-Questions/How-to-identify-stale-datanode/td-p/96336
    dfs.namenode.stale.datanode.interval < last contact < (2 * dfs.namenode.heartbeat.recheck-interval))
    参数值如下：
    dfs.namenode.stale.datanode.interval=30000ms 当nn超过这个时间没收到dn的心跳时就标记该dn为stale，stale dn的读写优先级将下调
    last contact=nn上一次收到datanode心跳到当前时刻的时间间隔
    dfs.namenode.heartbeat.recheck-interval=300000ms 检查过期datanode的时间间隔，和dfs.heartbeat.interval(3s)一起还将检查datande是否stale
    https://www.cnblogs.com/sx66/p/12605059.html
    dn死亡的判定：如果dn与nn通信超时超过 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval 默认10.5分钟
    -->
    <property>
        <name>dfs.namenode.avoid.read.stale.datanode</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.avoid.write.stale.datanode</name>
        <value>true</value>
    </property>
    <!--委派hdfs授权的类-->
<!--    <property>
                <name>dfs.namenode.inode.attributes.provider.class</name>
        <value>org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer</value>
    </property>-->
    <!--If "true", the ContentSummary permission checking will use subAccess.
             If "false", the ContentSummary permission checking will NOT use subAccess.
    subAccess means using recursion to check the access of all descendants.-->
    <property>
        <name>dfs.permissions.ContentSummary.subAccess</name>
        <value>true</value>
    </property>
</configuration>

