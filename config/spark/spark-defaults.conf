#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
 spark.eventLog.enabled           true
 spark.eventLog.dir               hdfs://mobCluster01/sparklogs
 spark.serializer                 org.apache.spark.serializer.KryoSerializer
 spark.kryoserializer.buffer.max   1024m
 spark.eventLog.compress          true
 spark.driver.memory              2g
 spark.executor.extraJavaOptions  -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -XX:+PrintGCApplicationConcurrentTime -Xloggc:gc.log
 spark.default.parallelism        300
 
 spark.shuffle.service.port   7337
 spark.shuffle.file.buffer    128K

#若为true，在job结束后，将stage相关的文件保留而不是删除
spark.yarn.preserve.staging.files   false
spark.local.dir                     /hadoop/data/tmp/spark,/hadoop1/data/tmp/spark,/hadoop2/data/tmp/spark,/hadoop3/data/tmp/spark,/hadoop4/data/tmp/spark,/hadoop5/data/tmp/spark,/hadoop6/data/tmp/spark,/hadoop7/data/tmp/spark,/hadoop8/data/tmp/spark,/hadoop9/data/tmp/spark,/hadoop10/data/tmp/spark,/hadoop11/data/tmp/spark
#如果设置为”true”，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为”true”。在ext3中，因为文件系统的限制，这个选项可能机器（大于8核）降低效率
spark.shuffle.consolidateFiles      true
spark.shuffle.spill                 true
spark.yarn.historyServer.allowTracking      true
spark.yarn.jars      hdfs://mobCluster01/spark-jar/jars/*.jar

spark.sql.adaptive.enabled true
spark.dynamicAllocation.enabled true
spark.sql.shuffle.partitions 200
#当mapper端两个partition的数据合并后数据量小于targetPostShuffleInputSize时，Spark会将两个partition进行合并到一个reducer端进行处理。默认64m
spark.sql.adaptive.shuffle.targetPostShuffleInputSize 67108864
#当spark.sql.adaptive.enabled参数开启后，有时会导致很多分区被合并，为了防止分区过少而影响性能。设置该参数，保障至少的shuffle分区数
spark.sql.adaptive.minNumPostShufflePartitions 50
#控制在ORC切分时stripe的合并处理。当几个stripe的大小大于设定值时，会合并到一个task中处理。适当调小该值以增大读ORC表的并发 【最小大小的控制参数
spark.hadoop.mapreduce.input.fileinputformat.split.maxsize 134217728

spark.yarn.executor.memoryOverhead 1024
#spark为了优化读取parquet格式文件，使用自己的解析方式读取数据。将该方式置为false
spark.sql.hive.convertMetastoreParquet false

#资源动态分配
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 720
spark.dynamicAllocation.schedulerBacklogTimeout 1s 
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s
spark.shuffle.service.enabled  true
spark.dynamicAllocation.enabled true

spark.executor.extraLibraryPath $HADOOP_HOME/lib/native
spark.driver.extraLibraryPath $HADOOP_HOME/lib/native



